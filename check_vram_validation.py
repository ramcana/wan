#!/usr/bin/env python3
"""
Quick VRAM Validation Check
Tests if the VRAM validation error is resolved
"""

import torch
import json
from pathlib import Path

def check_cuda_status():
    """Check CUDA status and VRAM"""
    print("üîç CUDA Status Check")
    print("-" * 30)
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA not available!")
        print("   Suggestions:")
        print("   ‚Ä¢ Reinstall PyTorch with CUDA support")
        print("   ‚Ä¢ Check NVIDIA drivers")
        print("   ‚Ä¢ Verify GPU is properly connected")
        return False
    
    print("‚úÖ CUDA Available")
    print(f"   Version: {torch.version.cuda}")
    print(f"   PyTorch: {torch.__version__}")
    
    gpu_props = torch.cuda.get_device_properties(0)
    vram_gb = gpu_props.total_memory / (1024**3)
    
    print(f"   GPU: {gpu_props.name}")
    print(f"   VRAM: {vram_gb:.1f}GB")
    print(f"   Compute: {gpu_props.major}.{gpu_props.minor}")
    
    return True

def check_vram_usage():
    """Check current VRAM usage"""
    print("\nüíæ VRAM Usage Check")
    print("-" * 30)
    
    if not torch.cuda.is_available():
        print("‚ùå Cannot check VRAM - CUDA not available")
        return False
    
    try:
        total_memory = torch.cuda.get_device_properties(0).total_memory
        allocated_memory = torch.cuda.memory_allocated(0)
        
        total_gb = total_memory / (1024**3)
        used_gb = allocated_memory / (1024**3)
        free_gb = total_gb - used_gb
        
        print(f"‚úÖ Total VRAM: {total_gb:.1f}GB")
        print(f"   Used: {used_gb:.2f}GB")
        print(f"   Free: {free_gb:.1f}GB")
        print(f"   Usage: {(used_gb/total_gb)*100:.1f}%")
        
        if free_gb >= 10:
            print("‚úÖ Sufficient VRAM for generation")
        elif free_gb >= 6:
            print("‚ö†Ô∏è Moderate VRAM available - use lower resolutions")
        else:
            print("‚ùå Low VRAM available - may need optimization")
        
        return True
        
    except Exception as e:
        print(f"‚ùå VRAM check failed: {e}")
        return False

def check_backend_config():
    """Check backend configuration"""
    print("\n‚öôÔ∏è Backend Configuration Check")
    print("-" * 30)
    
    config_path = Path("backend/config.json")
    if not config_path.exists():
        print("‚ùå Backend config not found")
        return False
    
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        # Check generation mode
        gen_mode = config.get("generation", {}).get("mode", "unknown")
        if gen_mode == "real":
            print("‚úÖ Generation mode: Real AI")
        else:
            print(f"‚ö†Ô∏è Generation mode: {gen_mode}")
        
        # Check VRAM settings
        vram_limit = config.get("hardware", {}).get("vram_limit_gb", 0)
        print(f"‚úÖ VRAM limit: {vram_limit}GB")
        
        # Check offloading
        enable_offload = config.get("optimization", {}).get("enable_offload", True)
        if not enable_offload:
            print("‚úÖ CPU offload: Disabled (good for high VRAM)")
        else:
            print("‚ö†Ô∏è CPU offload: Enabled (may not be needed)")
        
        # Check quantization
        quantization = config.get("optimization", {}).get("quantization_level", "none")
        print(f"‚úÖ Quantization: {quantization}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Config check failed: {e}")
        return False

def check_frontend_config():
    """Check frontend VRAM configuration"""
    print("\nüé® Frontend Configuration Check")
    print("-" * 30)
    
    override_path = Path("frontend/gpu_override.json")
    if override_path.exists():
        try:
            with open(override_path, 'r') as f:
                config = json.load(f)
            
            gpu_name = config.get("gpu_name", "Unknown")
            total_vram = config.get("total_vram_gb", 0)
            usable_vram = config.get("usable_vram_gb", 0)
            
            print(f"‚úÖ GPU Override: {gpu_name}")
            print(f"‚úÖ Total VRAM: {total_vram:.1f}GB")
            print(f"‚úÖ Usable VRAM: {usable_vram:.1f}GB")
            
            # Check VRAM estimates
            estimates = config.get("vram_estimates", {})
            if estimates:
                print("‚úÖ VRAM Estimates:")
                for resolution, estimate in estimates.items():
                    if isinstance(estimate, dict):
                        base = estimate.get("base", 0)
                        print(f"   {resolution}: {base}GB base")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Frontend config check failed: {e}")
            return False
    else:
        print("‚ö†Ô∏è No GPU override found - using default detection")
        return False

def simulate_vram_validation():
    """Simulate VRAM validation for common settings"""
    print("\nüß™ VRAM Validation Simulation")
    print("-" * 30)
    
    if not torch.cuda.is_available():
        print("‚ùå Cannot simulate - CUDA not available")
        return False
    
    total_vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    
    # Test scenarios
    scenarios = [
        ("1280x720, 4s", 6.0),
        ("1920x1080, 4s", 10.0),
        ("1920x1080, 8s", 12.0),
        ("2560x1440, 4s", 14.0),
        ("2560x1440, 8s", 16.0)
    ]
    
    print(f"Available VRAM: {total_vram:.1f}GB")
    print("Validation Results:")
    
    for scenario, required_vram in scenarios:
        if required_vram <= total_vram - 2:  # 2GB buffer
            status = "‚úÖ PASS"
        elif required_vram <= total_vram:
            status = "‚ö†Ô∏è TIGHT"
        else:
            status = "‚ùå FAIL"
        
        print(f"   {scenario}: {required_vram:.1f}GB - {status}")
    
    return True

def main():
    """Main check function"""
    print("üîç VRAM VALIDATION CHECK")
    print("=" * 50)
    
    checks = [
        check_cuda_status,
        check_vram_usage,
        check_backend_config,
        check_frontend_config,
        simulate_vram_validation
    ]
    
    passed = 0
    for check in checks:
        try:
            if check():
                passed += 1
        except Exception as e:
            print(f"‚ùå Check failed: {e}")
    
    print(f"\nüìã SUMMARY: {passed}/{len(checks)} checks passed")
    
    if passed == len(checks):
        print("üéâ All checks passed! VRAM validation should work correctly.")
        print("\nüí° Next steps:")
        print("1. Start servers: start_optimized_rtx4080.bat")
        print("2. Test generation with 1920x1080, 4 seconds")
        print("3. Monitor VRAM usage during generation")
    elif passed >= 3:
        print("‚úÖ Most checks passed. System should work with minor issues.")
    else:
        print("‚ö†Ô∏è Several issues found. Check configuration and dependencies.")
    
    print(f"\nüîß If you still see 'Insufficient VRAM' errors:")
    print("‚Ä¢ Restart both frontend and backend servers")
    print("‚Ä¢ Clear browser cache and refresh")
    print("‚Ä¢ Check that gpu_override.json is being loaded")
    print("‚Ä¢ Verify PyTorch CUDA installation")

if __name__ == "__main__":
    main()