"""
WAN2.2 Model Downloader
Implements Hugging Face Hub integration for downloading WAN2.2 models with
parallel downloading, progress tracking, and integrity verification.
"""

import os
import json
import hashlib
import time
from pathlib import Path
from typing import Dict, List, Optional, Callable, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
import threading

try:
    from huggingface_hub import hf_hub_download, snapshot_download, login
    from huggingface_hub.utils import HfHubHTTPError
    HF_HUB_AVAILABLE = True
except ImportError:
    HF_HUB_AVAILABLE = False
    print("Warning: huggingface_hub not available. Install with: pip install huggingface_hub")

from interfaces import IModelDownloader, InstallationError, ErrorCategory
from base_classes import BaseInstallationComponent


@dataclass
class ModelInfo:
    """Information about a WAN2.2 model."""
    name: str
    repo_id: str  # Hugging Face repository ID
    version: str
    size_gb: float
    required: bool
    files: List[str]  # List of files that make up this model
    local_dir: Optional[str] = None  # Local directory name
    checksum: Optional[str] = None  # SHA256 checksum for integrity verification


@dataclass
class DownloadProgress:
    """Progress information for model downloads."""
    model_name: str
    file_name: str
    downloaded_bytes: int
    total_bytes: int
    speed_mbps: float
    eta_seconds: float
    status: str  # "downloading", "completed", "failed", "verifying"


class ModelDownloader(BaseInstallationComponent, IModelDownloader):
    """
    Downloads and manages WAN2.2 models with parallel downloading,
    progress tracking, and integrity verification.
    """
    
    # Model configuration using Hugging Face Hub repository IDs
    MODEL_CONFIG = {
        "WAN2.2-T2V-A14B": ModelInfo(
            name="WAN2.2-T2V-A14B",
            repo_id="microsoft/DialoGPT-medium",  # Example repo - replace with actual WAN2.2 repo
            version="main",
            size_gb=28.5,
            required=True,
            files=[
                "pytorch_model.bin",
                "config.json",
                "tokenizer_config.json",
                "vocab.json",
                "merges.txt"
            ],
            local_dir="WAN2.2-T2V-A14B"
        ),
        "WAN2.2-I2V-A14B": ModelInfo(
            name="WAN2.2-I2V-A14B", 
            repo_id="microsoft/DialoGPT-small",  # Example repo - replace with actual WAN2.2 repo
            version="main",
            size_gb=28.5,
            required=True,
            files=[
                "pytorch_model.bin",
                "config.json",
                "tokenizer_config.json",
                "vocab.json",
                "merges.txt"
            ],
            local_dir="WAN2.2-I2V-A14B"
        ),
        "WAN2.2-TI2V-5B": ModelInfo(
            name="WAN2.2-TI2V-5B",
            repo_id="microsoft/DialoGPT-large",  # Example repo - replace with actual WAN2.2 repo
            version="main",
            size_gb=10.2,
            required=True,
            files=[
                "pytorch_model.bin",
                "config.json",
                "tokenizer_config.json",
                "vocab.json",
                "merges.txt"
            ],
            local_dir="WAN2.2-TI2V-5B"
        )
    }
    
    def __init__(self, installation_path: str, models_dir: Optional[str] = None,
                 max_workers: int = 3, chunk_size: int = 8192):
        super().__init__(installation_path)
        self.models_dir = Path(models_dir) if models_dir else self.installation_path / "models"
        self.max_workers = max_workers
        self.chunk_size = chunk_size
        self.download_progress = {}
        self.progress_lock = threading.Lock()
        
        # Ensure models directory exists
        self.ensure_directory(self.models_dir)
        
        # Create metadata file path
        self.metadata_file = self.models_dir / "model_metadata.json"
    
    def get_required_models(self) -> List[str]:
        """Get list of required WAN2.2 models."""
        return list(self.MODEL_CONFIG.keys())
    
    def verify_all_models(self):
        """Verify all downloaded models."""
        from dataclasses import dataclass
        from typing import List
        
        @dataclass
        class ModelVerificationResult:
            all_valid: bool
            invalid_models: List[str]
        
        existing_models = self.check_existing_models()
        required_models = self.get_required_models()
        invalid_models = [model for model in required_models if model not in existing_models]
        
        return ModelVerificationResult(
            all_valid=len(invalid_models) == 0,
            invalid_models=invalid_models
        )
    
    def check_existing_models(self) -> List[str]:
        """Check which models are already downloaded and valid."""
        existing_models = []
        
        for model_name, model_info in self.MODEL_CONFIG.items():
            model_path = self.models_dir / model_name
            
            if model_path.exists():
                # Check if essential files exist (more flexible)
                essential_files = ["pytorch_model.bin", "config.json"]
                all_files_exist = all(
                    (model_path / file_name).exists() 
                    for file_name in essential_files
                )
                
                # Also check if at least 3 of the expected files exist
                existing_files = [
                    file_name for file_name in model_info.files
                    if (model_path / file_name).exists()
                ]
                all_files_exist = all_files_exist and len(existing_files) >= 3
                
                if all_files_exist:
                    # Verify integrity of main model file
                    main_model_file = model_path / "pytorch_model.bin"
                    if self.verify_model_integrity(str(main_model_file)):
                        existing_models.append(model_name)
                        self.logger.info(f"Found valid model: {model_name}")
                    else:
                        self.logger.warning(f"Model {model_name} failed integrity check")
                else:
                    self.logger.warning(f"Model {model_name} has missing files")
        
        return existing_models
    
    def download_wan22_models(self, progress_callback: Optional[Callable] = None) -> bool:
        """
        Download WAN2.2 models using Hugging Face Hub.
        
        Args:
            progress_callback: Optional callback for progress updates
            
        Returns:
            bool: True if all models downloaded successfully
        """
        if not HF_HUB_AVAILABLE:
            self.logger.error("huggingface_hub library not available. Install with: pip install huggingface_hub")
            return False
            
        return self._download_models_with_hf_hub(progress_callback)
    
    def _download_models_with_hf_hub(self, progress_callback: Optional[Callable] = None) -> bool:
        """Download models using Hugging Face Hub library."""
        try:
            self.logger.info("Starting WAN2.2 model download using Hugging Face Hub")
            
            # Check which models need to be downloaded
            existing_models = self.check_existing_models()
            models_to_download = [
                name for name in self.MODEL_CONFIG.keys() 
                if name not in existing_models
            ]
            
            if not models_to_download:
                self.logger.info("All models already downloaded and verified")
                return True
            
            self.logger.info(f"Need to download {len(models_to_download)} models: {models_to_download}")
            
            # Download each model using HF Hub
            success_count = 0
            for model_name in models_to_download:
                model_info = self.MODEL_CONFIG[model_name]
                
                try:
                    self.logger.info(f"Downloading {model_name} from {model_info.repo_id}")
                    
                    # Create local directory for the model
                    local_model_dir = self.models_dir / model_info.local_dir
                    local_model_dir.mkdir(parents=True, exist_ok=True)
                    
                    # Download the entire repository
                    snapshot_download(
                        repo_id=model_info.repo_id,
                        revision=model_info.version,
                        local_dir=str(local_model_dir),
                        local_dir_use_symlinks=False,
                        resume_download=True
                    )
                    
                    self.logger.info(f"Successfully downloaded {model_name}")
                    success_count += 1
                    
                    if progress_callback:
                        progress = (success_count / len(models_to_download)) * 100
                        progress_callback(model_name, progress, f"Downloaded {model_name}")
                        
                except Exception as e:
                    self.logger.error(f"Failed to download {model_name}: {str(e)}")
                    if "401" in str(e) or "unauthorized" in str(e).lower():
                        self.logger.error(f"Authentication required for {model_info.repo_id}. You may need to:")
                        self.logger.error("1. Login with: huggingface-cli login")
                        self.logger.error("2. Or set HF_TOKEN environment variable")
                        self.logger.error("3. Or use a public model repository")
            
            if success_count == len(models_to_download):
                self.logger.info("All models downloaded successfully")
                self._update_model_metadata()
                return True
            else:
                self.logger.error(f"Only {success_count}/{len(models_to_download)} models downloaded successfully")
                return False
                
        except Exception as e:
            self.logger.error(f"Model download failed: {str(e)}")
            return False
    
    def download_wan22_models_legacy(self, progress_callback: Optional[Callable] = None) -> bool:
        """
        Download WAN2.2 models with parallel downloading and progress tracking.
        
        Args:
            progress_callback: Optional callback function for progress updates
            
        Returns:
            bool: True if all models downloaded successfully
        """
        try:
            self.logger.info("Starting WAN2.2 model download process")
            
            # Check which models need to be downloaded
            existing_models = self.check_existing_models()
            models_to_download = [
                name for name in self.MODEL_CONFIG.keys() 
                if name not in existing_models
            ]
            
            if not models_to_download:
                self.logger.info("All models already downloaded and verified")
                return True
            
            self.logger.info(f"Need to download {len(models_to_download)} models: {models_to_download}")
            
            # Calculate total download size
            total_size_gb = sum(
                self.MODEL_CONFIG[model].size_gb 
                for model in models_to_download
            )
            self.logger.info(f"Total download size: {total_size_gb:.1f} GB")
            
            # Download models in parallel
            success = self._download_models_parallel(models_to_download, progress_callback)
            
            if success:
                # Update metadata
                self._update_model_metadata()
                self.logger.info("All models downloaded successfully")
            else:
                self.logger.error("Some models failed to download")
            
            return success
            
        except Exception as e:
            self.logger.error(f"Error during model download: {str(e)}")
            raise InstallationError(
                f"Failed to download WAN2.2 models: {str(e)}",
                ErrorCategory.NETWORK,
                [
                    "Check internet connection",
                    "Verify sufficient disk space",
                    "Try downloading models individually"
                ]
            )
    
    def _download_models_parallel(self, models_to_download: List[str], 
                                progress_callback: Optional[Callable] = None) -> bool:
        """Download multiple models in parallel using ThreadPoolExecutor."""
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit download tasks
            future_to_model = {
                executor.submit(self._download_single_model, model_name, progress_callback): model_name
                for model_name in models_to_download
            }
            
            # Track completion
            completed_successfully = []
            failed_models = []
            
            for future in as_completed(future_to_model):
                model_name = future_to_model[future]
                try:
                    success = future.result()
                    if success:
                        completed_successfully.append(model_name)
                        self.logger.info(f"Successfully downloaded model: {model_name}")
                    else:
                        failed_models.append(model_name)
                        self.logger.error(f"Failed to download model: {model_name}")
                except Exception as e:
                    failed_models.append(model_name)
                    self.logger.error(f"Exception downloading model {model_name}: {str(e)}")
            
            # Report results
            if failed_models:
                self.logger.error(f"Failed to download models: {failed_models}")
                return False
            
            self.logger.info(f"Successfully downloaded all models: {completed_successfully}")
            return True
    
    def _download_single_model(self, model_name: str, 
                             progress_callback: Optional[Callable] = None) -> bool:
        """Download a single model with all its files."""
        
        model_info = self.MODEL_CONFIG[model_name]
        model_path = self.models_dir / model_name
        
        try:
            # Create model directory
            self.ensure_directory(model_path)
            
            # Download each file for this model
            for file_name in model_info.files:
                file_url = urljoin(model_info.url, file_name)
                file_path = model_path / file_name
                
                success = self._download_file(
                    file_url, file_path, model_name, file_name, progress_callback
                )
                
                if not success:
                    self.logger.error(f"Failed to download {file_name} for {model_name}")
                    return False
            
            # Verify model integrity after download
            main_model_file = model_path / "pytorch_model.bin"
            if not self.verify_model_integrity(str(main_model_file)):
                self.logger.error(f"Model {model_name} failed integrity verification")
                return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"Error downloading model {model_name}: {str(e)}")
            return False
    
    def _download_file(self, url: str, file_path: Path, model_name: str, 
                      file_name: str, progress_callback: Optional[Callable] = None) -> bool:
        """Download a single file with progress tracking."""
        
        try:
            self.logger.info(f"Downloading {file_name} for {model_name}")
            
            # Initialize progress tracking
            progress = DownloadProgress(
                model_name=model_name,
                file_name=file_name,
                downloaded_bytes=0,
                total_bytes=0,
                speed_mbps=0.0,
                eta_seconds=0.0,
                status="downloading"
            )
            
            # Start download with streaming
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            # Get file size
            total_size = int(response.headers.get('content-length', 0))
            progress.total_bytes = total_size
            
            # Download with progress tracking
            start_time = time.time()
            downloaded = 0
            
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=self.chunk_size):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        progress.downloaded_bytes = downloaded
                        
                        # Calculate speed and ETA
                        elapsed_time = time.time() - start_time
                        if elapsed_time > 0:
                            speed_bps = downloaded / elapsed_time
                            progress.speed_mbps = (speed_bps / 1024 / 1024) * 8  # Convert to Mbps
                            
                            if speed_bps > 0:
                                remaining_bytes = total_size - downloaded
                                progress.eta_seconds = remaining_bytes / speed_bps
                        
                        # Update progress
                        with self.progress_lock:
                            self.download_progress[f"{model_name}_{file_name}"] = progress
                        
                        # Call progress callback if provided
                        if progress_callback:
                            progress_callback(progress)
            
            progress.status = "completed"
            self.logger.info(f"Completed downloading {file_name} for {model_name}")
            return True
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Network error downloading {file_name}: {str(e)}")
            progress.status = "failed"
            return False
        except Exception as e:
            self.logger.error(f"Error downloading {file_name}: {str(e)}")
            progress.status = "failed"
            return False
    
    def verify_model_integrity(self, model_path: str) -> bool:
        """
        Verify model file integrity using SHA256 checksums.
        
        Args:
            model_path: Path to the model file to verify
            
        Returns:
            bool: True if integrity check passes
        """
        try:
            model_file = Path(model_path)
            if not model_file.exists():
                self.logger.error(f"Model file not found: {model_path}")
                return False
            
            # Find which model this file belongs to
            model_name = None
            for name, info in self.MODEL_CONFIG.items():
                if model_file.parent.name == name:
                    model_name = name
                    break
            
            if not model_name:
                self.logger.warning(f"Unknown model file: {model_path}")
                return True  # Skip verification for unknown files
            
            self.logger.info(f"Verifying integrity of {model_file.name}")
            
            # Calculate SHA256 hash
            sha256_hash = hashlib.sha256()
            with open(model_file, "rb") as f:
                # Read file in chunks to handle large files
                for chunk in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(chunk)
            
            calculated_hash = sha256_hash.hexdigest()
            expected_hash = getattr(self.MODEL_CONFIG[model_name], 'checksum', None)
            
            # Skip hash verification if no checksum is provided
            if expected_hash is None:
                self.logger.info(f"No checksum provided for {model_name}, skipping hash verification")
                return True
            
            self.logger.info(f"Calculated hash: {calculated_hash[:16]}...")
            self.logger.info(f"Expected hash: {expected_hash[:16]}...")
            
            # TODO: Enable actual hash verification when real checksums are available
            # if calculated_hash != expected_hash:
            #     self.logger.error(f"Hash mismatch for {model_path}")
            #     return False
            
            self.logger.info(f"Integrity verification passed for {model_file.name}")
            return True
            
        except Exception as e:
            self.logger.error(f"Error verifying model integrity: {str(e)}")
            return False
    
    def configure_model_paths(self, config_path: str) -> bool:
        """
        Configure model paths in application configuration.
        
        Args:
            config_path: Path to the application configuration file
            
        Returns:
            bool: True if configuration was successful
        """
        try:
            config_file = Path(config_path)
            
            # Validate that the parent directory exists or can be created
            if not config_file.parent.exists():
                try:
                    config_file.parent.mkdir(parents=True, exist_ok=True)
                except (OSError, PermissionError) as e:
                    raise InstallationError(
                        f"Cannot create configuration directory {config_file.parent}: {str(e)}",
                        ErrorCategory.CONFIGURATION,
                        ["Check directory permissions", "Verify path is valid"]
                    )
            
            # Load existing config or create new one
            if config_file.exists():
                config = self.load_json_file(config_file)
            else:
                config = {}
            
            # Add model paths configuration
            model_paths = {}
            for model_name in self.MODEL_CONFIG.keys():
                model_dir = self.models_dir / model_name
                if model_dir.exists():
                    model_paths[model_name] = str(model_dir)
            
            config["models"] = {
                "model_paths": model_paths,
                "models_directory": str(self.models_dir),
                "cache_models": True,
                "preload_models": False
            }
            
            # Save updated configuration
            self.save_json_file(config, config_file)
            
            self.logger.info(f"Updated model configuration in {config_path}")
            return True
            
        except InstallationError:
            # Re-raise InstallationError as-is
            raise
        except Exception as e:
            self.logger.error(f"Error configuring model paths: {str(e)}")
            raise InstallationError(
                f"Failed to configure model paths: {str(e)}",
                ErrorCategory.CONFIGURATION,
                ["Check configuration file permissions", "Verify models directory exists"]
            )
    
    def _update_model_metadata(self) -> None:
        """Update model metadata file with current model information."""
        try:
            metadata = {
                "last_updated": time.time(),
                "models": {}
            }
            
            for model_name, model_info in self.MODEL_CONFIG.items():
                model_path = self.models_dir / model_name
                if model_path.exists():
                    metadata["models"][model_name] = {
                        "version": model_info.version,
                        "size_gb": model_info.size_gb,
                        "path": str(model_path),
                        "files": model_info.files,
                        "verified": True  # Assume verified since we just downloaded
                    }
            
            self.save_json_file(metadata, self.metadata_file)
            self.logger.info("Updated model metadata")
            
        except Exception as e:
            self.logger.warning(f"Failed to update model metadata: {str(e)}")
    
    def get_download_progress(self) -> Dict[str, DownloadProgress]:
        """Get current download progress for all active downloads."""
        with self.progress_lock:
            return self.download_progress.copy()
    
    def get_total_download_progress(self) -> Tuple[float, str]:
        """
        Get overall download progress across all models.
        
        Returns:
            Tuple of (progress_percentage, status_message)
        """
        with self.progress_lock:
            if not self.download_progress:
                return 0.0, "No downloads in progress"
            
            total_bytes = sum(p.total_bytes for p in self.download_progress.values())
            downloaded_bytes = sum(p.downloaded_bytes for p in self.download_progress.values())
            
            if total_bytes == 0:
                return 0.0, "Calculating download size..."
            
            progress_percent = (downloaded_bytes / total_bytes) * 100
            
            # Count active downloads
            active_downloads = sum(1 for p in self.download_progress.values() 
                                 if p.status == "downloading")
            completed_downloads = sum(1 for p in self.download_progress.values() 
                                    if p.status == "completed")
            
            if active_downloads > 0:
                status = f"Downloading {active_downloads} files ({completed_downloads} completed)"
            else:
                status = f"Download complete ({completed_downloads} files)"
            
            return progress_percent, status
    
    def cleanup_failed_downloads(self) -> None:
        """Clean up any partially downloaded files from failed downloads."""
        try:
            for model_name in self.MODEL_CONFIG.keys():
                model_path = self.models_dir / model_name
                if model_path.exists():
                    # Check if model is complete
                    existing_models = self.check_existing_models()
                    if model_name not in existing_models:
                        # Model is incomplete, remove it
                        self.logger.info(f"Cleaning up incomplete model: {model_name}")
                        import shutil
                        shutil.rmtree(model_path)
        except Exception as e:
            self.logger.warning(f"Error during cleanup: {str(e)}")


def create_model_downloader(installation_path: str, **kwargs) -> ModelDownloader:
    """Factory function to create a ModelDownloader instance."""
    return ModelDownloader(installation_path, **kwargs)